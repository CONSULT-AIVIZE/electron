'use client'

import React, { useState, useEffect, useCallback, useRef } from 'react'
import { useRouter } from 'next/navigation'
import { RuntimeService } from '../core/runtime/RuntimeService'
import type { RuntimeCommand } from '../core/runtime/RuntimeService'
import { appRegistry } from '../core/config/appConfig'
import { getStartupAppConfig, initializeDefaultApps } from '../core/config/defaultApps'

interface VoiceAssistantProps {
  className?: string
}

const VoiceAssistant: React.FC<VoiceAssistantProps> = ({ className = '' }) => {
  const router = useRouter()
  const [isListening, setIsListening] = useState(false)
  const [isSupported, setIsSupported] = useState(false)
  const [lastCommand, setLastCommand] = useState('')
  const [transcript, setTranscript] = useState('')
  const [error, setError] = useState('')
  const [availableCommands, setAvailableCommands] = useState<RuntimeCommand[]>([])
  const [runtimeService] = useState(() => new RuntimeService())
  
  const recognitionRef = useRef<SpeechRecognition | null>(null)
  const restartTimeoutRef = useRef<NodeJS.Timeout | null>(null)

  // Initialize runtime service with default app
  useEffect(() => {
    const initializeRuntime = async () => {
      try {
        // é¦–å…ˆåˆå§‹åŒ–é»˜è®¤åº”ç”¨
        await initializeDefaultApps()
        
        const appConfig = getStartupAppConfig()
        if (!appConfig) {
          console.error('âŒ æ²¡æœ‰å¯ç”¨çš„åº”ç”¨é…ç½®')
          return
        }
        console.log('ğŸš€ è¯­éŸ³åŠ©æ‰‹ï¼šåˆå§‹åŒ–è¿è¡Œæ—¶æœåŠ¡')
        console.log('ğŸ“± åº”ç”¨é…ç½®:', appConfig)
        
        const success = await runtimeService.loadApp({
          id: appConfig.id,
          name: appConfig.name,
          url: appConfig.url,
          type: 'website',
          features: {
            voice_control: true,
            ai_styling: true,
            traditional_mode: true,
            adaptive_ui: true
          }
        })
        
        if (success) {
          console.log('âœ… è¿è¡Œæ—¶æœåŠ¡åˆå§‹åŒ–æˆåŠŸ')
          // è·å–å¯ç”¨æŒ‡ä»¤
          const commands = runtimeService.getCurrentCommands()
          setAvailableCommands(commands)
          console.log('ğŸ“‹ å·²åŠ è½½æŒ‡ä»¤:', commands.length, 'ä¸ª')
          
          // ç›‘å¬æŒ‡ä»¤å˜åŒ–
          runtimeService.addListener((newCommands) => {
            setAvailableCommands(newCommands)
            console.log('ğŸ”„ æŒ‡ä»¤åˆ—è¡¨å·²æ›´æ–°:', newCommands.length, 'ä¸ª')
          })
        } else {
          console.error('âŒ è¿è¡Œæ—¶æœåŠ¡åˆå§‹åŒ–å¤±è´¥')
        }
      } catch (error) {
        console.error('ğŸ’¥ è¿è¡Œæ—¶æœåŠ¡åˆå§‹åŒ–å¼‚å¸¸:', error)
      }
    }
    
    initializeRuntime()
  }, [runtimeService])

  // Initialize speech recognition
  useEffect(() => {
    if (typeof window !== 'undefined') {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition
      if (SpeechRecognition) {
        setIsSupported(true)
        const recognition = new SpeechRecognition()
        
        // Configure recognition settings
        recognition.continuous = true
        recognition.interimResults = true
        recognition.lang = 'zh-CN' // Primary language: Chinese
        recognition.maxAlternatives = 3

        // Event handlers
        recognition.onstart = () => {
          setIsListening(true)
          setError('')
          console.log('Voice recognition started')
        }

        recognition.onresult = (event) => {
          let currentTranscript = ''
          let finalTranscript = ''

          for (let i = event.resultIndex; i < event.results.length; i++) {
            const result = event.results[i]
            if (result.isFinal) {
              finalTranscript += result[0].transcript
            } else {
              currentTranscript += result[0].transcript
            }
          }

          setTranscript(currentTranscript)
          
          if (finalTranscript) {
            console.log('Final transcript:', finalTranscript)
            processVoiceCommand(finalTranscript.trim())
            setLastCommand(finalTranscript.trim())
          }
        }

        recognition.onerror = (event) => {
          console.error('Speech recognition error:', event.error)
          
          // Handle different error types
          if (event.error === 'network') {
            setError('ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œè¯­éŸ³è¯†åˆ«æš‚æ—¶ä¸å¯ç”¨')
            // Don't auto-restart network errors in development
            setIsListening(false)
            return
          } else if (event.error === 'no-speech') {
            // This is normal, just restart
            scheduleRestart()
            return
          } else if (event.error === 'audio-capture') {
            setError('æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®')
          } else if (event.error === 'not-allowed') {
            setError('éº¦å…‹é£æƒé™è¢«æ‹’ç»ï¼Œè¯·åœ¨è®¾ç½®ä¸­å…è®¸éº¦å…‹é£è®¿é—®')
          } else {
            setError(`è¯­éŸ³è¯†åˆ«é”™è¯¯: ${event.error}`)
          }
          
          setIsListening(false)
        }

        recognition.onend = () => {
          console.log('Voice recognition ended')
          setIsListening(false)
          setTranscript('')
          
          // Auto-restart if not manually stopped
          if (recognitionRef.current && isListening) {
            scheduleRestart()
          }
        }

        recognitionRef.current = recognition
      } else {
        setIsSupported(false)
        setError('Speech recognition not supported in this browser')
      }
    }

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop()
      }
      if (restartTimeoutRef.current) {
        clearTimeout(restartTimeoutRef.current)
      }
    }
  }, [])

  // Schedule recognition restart
  const scheduleRestart = useCallback(() => {
    if (restartTimeoutRef.current) {
      clearTimeout(restartTimeoutRef.current)
    }
    
    restartTimeoutRef.current = setTimeout(() => {
      if (recognitionRef.current && !isListening) {
        try {
          recognitionRef.current.start()
        } catch (error) {
          console.error('Failed to restart recognition:', error)
        }
      }
    }, 1000) // Restart after 1 second
  }, [isListening])

  // Process voice commands
  const processVoiceCommand = useCallback(async (command: string) => {
    const lowerCommand = command.toLowerCase()
    console.log('Processing command:', lowerCommand)

    // First try to match against loaded runtime commands
    const matchedCommand = runtimeService.matchCommand(command)
    if (matchedCommand) {
      console.log('Matched runtime command:', matchedCommand)
      const success = await runtimeService.executeCommand(matchedCommand)
      if (success) {
        console.log('âœ… Runtime command executed successfully')
        return
      }
    }

    // Fallback to built-in navigation commands (Chinese and English)
    const navigationCommands = [
      { patterns: ['æ‰“å¼€å’¨è¯¢', 'å’¨è¯¢', 'å¼€å§‹å’¨è¯¢', 'open consult', 'consultation', 'consult'], route: '/consult' },
      { patterns: ['æ‰“å¼€è®¾ç½®', 'è®¾ç½®', 'ç³»ç»Ÿè®¾ç½®', 'open settings', 'settings', 'preferences'], route: '/settings' },
      { patterns: ['å›åˆ°ä¸»é¡µ', 'ä¸»é¡µ', 'æ¡Œé¢', 'home', 'dashboard', 'go home'], route: '/' },
    ]

    // Check navigation commands
    for (const nav of navigationCommands) {
      if (nav.patterns.some(pattern => lowerCommand.includes(pattern))) {
        router.push(nav.route)
        return
      }
    }

    // System commands
    if (lowerCommand.includes('æœ€å°åŒ–') || lowerCommand.includes('minimize')) {
      if (window.electronAPI) {
        window.electronAPI.minimizeApp()
      }
      return
    }

    if (lowerCommand.includes('å…¨å±') || lowerCommand.includes('fullscreen')) {
      if (window.electronAPI) {
        window.electronAPI.toggleFullscreen()
      }
      return
    }

    // Send to Electron for system-level handling
    if (window.electronAPI) {
      window.electronAPI.sendVoiceCommand(command)
    }
  }, [router, runtimeService])

  // Toggle listening
  const toggleListening = useCallback(() => {
    if (!recognitionRef.current || !isSupported) return

    if (isListening) {
      recognitionRef.current.stop()
      setIsListening(false)
    } else {
      try {
        recognitionRef.current.start()
      } catch (error) {
        console.error('Failed to start recognition:', error)
        setError('Failed to start voice recognition')
      }
    }
  }, [isListening, isSupported])

  // Auto-start listening (only if not in error state)
  useEffect(() => {
    if (isSupported && recognitionRef.current && !isListening && !error) {
      const timer = setTimeout(() => {
        try {
          recognitionRef.current?.start()
        } catch (error) {
          console.error('Auto-start failed:', error)
        }
      }, 2000)
      
      return () => clearTimeout(timer)
    }
  }, [isSupported, isListening, error])

  // This effect is handled by the main initialization above

  // Always render the interface, but show appropriate state when not supported

  return (
    <div className={`h-full w-full flex items-center justify-center px-8 ${className || ''}`}>
      <div className="flex items-center space-x-8 w-full max-w-4xl">
        {/* è¯­éŸ³æŒ‰é’® */}
        <button
          onClick={toggleListening}
          className={`
            w-16 h-16 rounded-full shadow-2xl transition-all duration-500 flex items-center justify-center relative flex-shrink-0
            ${!isSupported 
              ? 'bg-gray-600 cursor-not-allowed' 
              : isListening 
                ? 'bg-red-500 hover:bg-red-400' 
                : 'bg-gradient-to-r from-blue-500 to-purple-600 hover:from-blue-400 hover:to-purple-500'
            }
            ${isSupported ? 'hover:scale-110 active:scale-95' : ''}
          `}
          title={!isSupported ? 'è¯­éŸ³è¯†åˆ«ä¸æ”¯æŒ' : isListening ? 'åœæ­¢è¯­éŸ³è¯†åˆ«' : 'å¼€å§‹è¯­éŸ³æ§åˆ¶'}
          disabled={!isSupported}
        >
          {!isSupported ? (
            <svg className="w-8 h-8 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z M13 13l6 6" />
            </svg>
          ) : isListening ? (
            <svg className="w-8 h-8 text-white" fill="currentColor" viewBox="0 0 24 24">
              <path d="M6 6h12v12H6z"/>
            </svg>
          ) : (
            <svg className="w-8 h-8 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
            </svg>
          )}
          
          {/* è¯­éŸ³æŒ‡ç¤ºç¯ */}
          {isListening && (
            <>
              <div className="absolute inset-0 rounded-full border-2 border-red-300 animate-ping"></div>
              <div className="absolute -inset-2 rounded-full border border-red-200 animate-pulse"></div>
              <div className="absolute -inset-4 rounded-full border border-red-100 opacity-50 animate-pulse" style={{animationDelay: '0.5s'}}></div>
            </>
          )}
        </button>
        
        {/* è½¬å½•æ˜¾ç¤ºåŒº */}
        <div className="flex-1 min-h-[80px] flex items-center">
          {transcript ? (
            <div className="text-white text-xl font-light leading-relaxed animate-pulse">
              {transcript}
              <span className="animate-pulse ml-1 text-blue-400">|</span>
            </div>
          ) : lastCommand ? (
            <div className="text-white text-lg font-light leading-relaxed opacity-80">
              æœ€åæŒ‡ä»¤: {lastCommand}
            </div>
          ) : (
            <div className="text-white text-base opacity-40">
              {!isSupported ? 'è¯­éŸ³è¯†åˆ«åœ¨å½“å‰ç¯å¢ƒä¸‹ä¸å¯ç”¨' : 
               isListening ? 'è¯·å¼€å§‹è¯´è¯...' : 'ç‚¹å‡»éº¦å…‹é£å¼€å§‹è¯­éŸ³äº¤äº’'}
            </div>
          )}
          
          {/* é”™è¯¯æ˜¾ç¤º */}
          {error && (
            <div className="ml-4 text-red-400 text-sm bg-red-500 bg-opacity-10 rounded-lg px-3 py-1 backdrop-blur-sm">
              {error}
            </div>
          )}
        </div>

        {/* çŠ¶æ€æŒ‡ç¤º */}
        <div className="flex items-center space-x-2 flex-shrink-0">
          <div className={`w-3 h-3 rounded-full ${
            !isSupported ? 'bg-gray-400' : 
            isListening ? 'bg-red-400 animate-pulse' : 'bg-white bg-opacity-30'
          }`}></div>
          <span className="text-white text-sm opacity-70 whitespace-nowrap">
            {!isSupported ? 'ä¸å¯ç”¨' : isListening ? 'ç›‘å¬ä¸­' : 'å¾…å‘½'}
          </span>
        </div>
      </div>
    </div>
  )
}

// Extend Window interface for TypeScript
declare global {
  interface Window {
    SpeechRecognition: typeof SpeechRecognition
    webkitSpeechRecognition: typeof SpeechRecognition
  }
}

export default VoiceAssistant